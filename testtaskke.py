# -*- coding: utf-8 -*-
"""TestTaskKE.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BiaXFV-kqmfFayLZ6qFHx1BCtXWexAyd
"""


import s3fs

# Подключение до S3
fs = s3fs.S3FileSystem(key="oGG7onz8X7dfT_AKvwKn",
                       secret="YT74LUkrzipAFWcBmHuBOszIxwdLFtjNPkxkVV4O",
                       client_kwargs={'endpoint_url': "http://storage.yandexcloud.net"})

with fs.open(f"nlp-test-task/data.csv", "rb") as remote_file:
    # Скачает датасет в файл data.csv 
    with open("data.csv", "wb") as local_file:
        local_file.write(remote_file.read())

import pandas as pd

df = pd.read_csv("data.csv")

df.head()

from bs4 import BeautifulSoup

def clean(html):
    soup = BeautifulSoup(str(html), 'html')
    return soup.text

df['description_clean'] = df['description'].apply(clean)

df.head()

"""###Description len destribution (kinda exponential)"""

import matplotlib.pyplot as plt
fig, ax = plt.subplots()
ax.set_xscale('log')

df['description_clean'].str.len().hist(bins=100)

"""###Title len destribution (kinda normal lol)"""

fig, ax = plt.subplots()
ax.set_xscale('log')

df['title'].str.len().hist(bins=100)

import nltk
from nltk.corpus import stopwords
from nltk.stem.snowball import SnowballStemmer

nltk.download('punkt')
nltk.download('stopwords')


def preprocessing(text: str) -> str:
    text = str(text)
    # tokenize sent
    tokens = nltk.word_tokenize(text)
    # remove non alphabetic words
    words = [word.lower() for word in tokens if word.isalpha()]

    # remove punctuation
    import string
    table = str.maketrans('', '', string.punctuation)
    words = [w.translate(table) for w in words]

    # remove stop words
    stop_words = set(stopwords.words('russian'))
    words = [w for w in words if not w in stop_words]


    # test
    porter = SnowballStemmer("russian")
    stemmed = [porter.stem(word) for word in words]

    return ' '.join(stemmed)

preprocessing('Защитное стекло, iPhone 5/5S/SE Черное | 10D с ..')

df_ready = df[["title", "description_clean"]]
df_ready = df_ready.applymap(preprocessing)
df_ready.head()

df_ready.to_csv('df_ready.csv')

import pandas as pd
df_ready = pd.read_csv("df_ready.csv").dropna()
df_ready

"""## Предесловие
ембединги получаются путем прогона всех текстов через предобученные модели-трансформеры.



"""## Prepare inference model V2 (launch in separete session fomr V1 (tf incompat))"""


import tensorflow_hub as hub
import tensorflow as tf
import numpy as np
import tensorflow_text as text 
embed = hub.load("https://tfhub.dev/google/universal-sentence-encoder-multilingual/3")

df_ready = df.dropna()

from tqdm import tqdm_notebook

vectors = []
batch = []
batch_size = 128

for index, row in tqdm_notebook(df_ready.iterrows()):
    batch.append([row['title'] + ' ' + row['description_clean']])
    
    if len(batch) == batch_size:

        vectors.append(embed(batch))
        batch = []


vectors.append(embed(batch))
vectors = np.concatenate([i.numpy() for i in vectors], axis=0)

vectors.shape

def get_vector(query:str) -> np.array:
    return embed([query])[0].numpy()

"""## Построить векторное представление"""

def normalization(embeds):
  norms = np.linalg.norm(embeds, 2, axis=1, keepdims=True)
  return embeds/norms

vectors_norm = normalization(vectors)
vectors_norm.shape

"""изначальная идея была использовать `TSNE` но из-за большого количества данных был выбран `TruncatedSVD`, так как он работает быстрее"""

from sklearn.manifold import TSNE
from sklearn.decomposition import TruncatedSVD

encoded = TruncatedSVD(n_components=2).fit_transform(vectors_norm)

encoded.shape

"""## Показать проекцию всех товаров, или части товаров, на 2D"""

import numpy as np
import matplotlib.pyplot as plt


x = encoded[:,0]
y = encoded[:,1]

plt.scatter(x, y)
plt.show()

"""## Показать примеры решения задач кластеризации с помощью этих представлений

среди алгоритмов кластеризации был выбран `MiniBatchKMeans` из-за его скорости и возможности создания кластеров в равномерно распределенных данных
"""

from sklearn.cluster import MiniBatchKMeans, KMeans, SpectralClustering
import matplotlib.cm as cm

model = MiniBatchKMeans(20, batch_size=1000)
prediction = model.fit_predict(vectors_norm)

x = encoded[:,0]
y = encoded[:,1]

plt.scatter(x, y, c=prediction, cmap=cm.jet)
plt.show()

"""## Показать примеры решения задач поиска K ближайших с помощью этих представлений

изначальная идея ембедингов составить скрытое пространство дл япредставления схожести документов как расстояние в этом пространстве. Для задачи поиска в таком пространстве подходящий вариант `NearestNeighbors` для поиска K ближайщих соседей в этом пространстве.
"""

k = 5

from sklearn.neighbors import NearestNeighbors

nbrs = NearestNeighbors(n_neighbors=k, algorithm='ball_tree').fit(vectors_norm)

query = 'свитер'
# query = preprocessing(query)

query_vec = get_vector(query)

distances, indices = nbrs.kneighbors([query_vec])

df_ready.iloc[indices[0]]

from typing import List

def get_5_nearest(query: str) -> List[str]:

    query_vec = get_vector(query)

    distances, indices = nbrs.kneighbors([query_vec])

    result = []
    for i, row in df_ready.iloc[indices[0]].iterrows():

        result.append(row['title'] + ' ' + row['description_clean'])

    return result

query = 'свитер'
get_5_nearest(query)
